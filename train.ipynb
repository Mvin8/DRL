{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mvin/Code/DRL-and-graph-neural-network-for-routing-problems/vrptest/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "from VRP.VRP_PPO_Model import Agentppo,Memory\n",
    "from VRP.creat_vrp import creat_data,reward,reward1\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "n_nodes =21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(model, dataset,batch_size,steps):\n",
    "\n",
    "    model.eval()\n",
    "    def eval_model_bat(bat):\n",
    "        with torch.no_grad():\n",
    "            cost, _ = model.act(bat,0,steps,batch_size,True,False)\n",
    "\n",
    "            cost = reward1(bat.x,cost.detach(), n_nodes)\n",
    "        return cost.cpu()\n",
    "    totall_cost = torch.cat([eval_model_bat(bat.to(device))for bat in dataset], 0)\n",
    "    return totall_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainPPO:\n",
    "    def __init__(self,steps,greedy,lr,input_node_dim,hidden_node_dim,input_edge_dim,\n",
    "                 hidden_edge_dim,epoch=2,batch_size=32,conv_laysers=3,entropy_value=0.01,eps_clip=0.2,timestep=4,ppo_epoch=2):\n",
    "\n",
    "        self.steps = steps\n",
    "        self.greedy = greedy\n",
    "        self.batch_size = batch_size\n",
    "        self.update_timestep = timestep\n",
    "        self.epoch =epoch\n",
    "        self.memory = Memory()\n",
    "        self.agent = Agentppo(steps,greedy,lr,input_node_dim,hidden_node_dim,\n",
    "                              input_edge_dim,hidden_edge_dim,ppo_epoch,batch_size,conv_laysers,entropy_value,eps_clip)\n",
    "\n",
    "    def run_train(self,data_loader,batch_size,valid_loder):\n",
    "        memory = Memory()\n",
    "        self.agent.old_polic.to(device)\n",
    "        #initWeights(self.agent.old_polic)\n",
    "        #initWeights(self.agent.policy)\n",
    "        folder = 'vrp-{}-GAT'.format(n_nodes)\n",
    "        filename = '20201125'\n",
    "        filepath = os.path.join(folder, filename)\n",
    "\n",
    "        '''path = os.path.join(filepath,'%s' % 3)\n",
    "        if os.path.exists(path):\n",
    "            path1 = os.path.join(path, 'actor.pt')\n",
    "            self.agent.old_polic.load_state_dict(torch.load(path1, device))'''\n",
    "\n",
    "        costs = []\n",
    "        for i in range(self.epoch):\n",
    "            print('old_epoch:', i, '***************************************')\n",
    "            self.agent.old_polic.train()\n",
    "            times, losses, rewards2, critic_rewards = [], [], [], []\n",
    "            epoch_start = time.time()\n",
    "            start = epoch_start\n",
    "            for batch_idx, batch in enumerate(data_loader):\n",
    "\n",
    "                x,attr,capcity,demand = batch.x,batch.edge_attr,batch.capcity,batch.demand\n",
    "                #print(x.size(),index.size(),attr.size())\n",
    "                x,attr,capcity,demand = x.view(batch_size,n_nodes,2),attr.view(batch_size,n_nodes*n_nodes,1),capcity.view(batch_size,1),demand.view(batch_size,n_nodes,1)\n",
    "                batch = batch.to(device)\n",
    "                actions, log_p = self.agent.old_polic.act(batch,0,self.steps,batch_size,self.greedy,False)\n",
    "\n",
    "                rewards = reward1(batch.x, actions, n_nodes)\n",
    "\n",
    "                actions = actions.to(torch.device('cpu')).detach()\n",
    "                log_p = log_p.to(torch.device('cpu')).detach()\n",
    "                rewards = rewards.to(torch.device('cpu')).detach()\n",
    "\n",
    "                #print(actions.size(),log_p.size(),entropy.size())\n",
    "\n",
    "                for i_batch in range(self.batch_size):\n",
    "                    memory.input_x.append(x[i_batch])\n",
    "                    #memory.input_index.append(index[i_batch])\n",
    "                    memory.input_attr.append(attr[i_batch])\n",
    "                    memory.actions.append(actions[i_batch])\n",
    "                    memory.log_probs.append(log_p[i_batch])\n",
    "                    memory.rewards.append(rewards[i_batch])\n",
    "                    memory.capcity.append(capcity[i_batch])\n",
    "                    memory.demand.append(demand[i_batch])\n",
    "                if (batch_idx+1)%self.update_timestep == 0:\n",
    "                    self.agent.update(memory,i)\n",
    "                    memory.def_memory()\n",
    "                rewards2.append(torch.mean(rewards.detach()).item())\n",
    "                time_Space = 100\n",
    "                if (batch_idx+1) % time_Space == 0:\n",
    "                    end = time.time()\n",
    "                    times.append(end - start)\n",
    "                    start = end\n",
    "                    mean_reward = np.mean(rewards2[-time_Space:])\n",
    "                    print('  Batch %d/%d, reward: %2.3f,took: %2.4fs' %\n",
    "                          (batch_idx, len(data_loader), mean_reward,\n",
    "                           times[-1]))\n",
    "            cost = rollout(self.agent.policy, valid_loder, batch_size, self.steps)\n",
    "            cost = cost.mean()\n",
    "            costs.append(cost.item())\n",
    "            print('Problem:TSP''%s' % n_nodes,'/ Average distance:',cost.item())\n",
    "            print(costs)\n",
    "            epoch_dir = os.path.join(filepath, '%s' % i)\n",
    "            if not os.path.exists(epoch_dir):\n",
    "                os.makedirs(epoch_dir)\n",
    "            save_path = os.path.join(epoch_dir, 'actor.pt')\n",
    "            torch.save(self.agent.old_polic.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    class RunBuilder():\n",
    "        @staticmethod\n",
    "        def get_runs(params):\n",
    "            Run = namedtuple('Run', params.keys())\n",
    "            runs = []\n",
    "            for v in product(*params.values()):\n",
    "                runs.append(Run(*v))\n",
    "            return runs\n",
    "\n",
    "    params = OrderedDict(\n",
    "        lr=[3e-4],\n",
    "        hidden_node_dim=[128],\n",
    "        hidden_edge_dim=[16],\n",
    "        epoch = [100],\n",
    "        batch_size=[512],\n",
    "        conv_laysers=[4],\n",
    "        entropy_value=[0.01],\n",
    "        eps_clip=[0.2],\n",
    "        timestep=[1],\n",
    "        ppo_epoch=[3],\n",
    "        data_size=[512000],\n",
    "        valid_size=[10000]\n",
    "    )\n",
    "    runs = RunBuilder.get_runs(params)\n",
    "\n",
    "    for lr, hidden_node_dim, hidden_edge_dim, epoch,batch_size,conv_laysers,entropy_value,eps_clip,timestep,ppo_epoch ,data_size,valid_size in runs:\n",
    "        print('lr', 'batch_size', 'hidden_node_dim', 'hidden_edge_dim', 'conv_laysers', 'epoch,batch_size',\n",
    "              'entropy_value', 'eps_clip', 'timestep:','data_size','valid_size', lr, hidden_node_dim,\n",
    "              hidden_edge_dim, epoch, batch_size, conv_laysers, entropy_value, eps_clip, timestep,data_size,valid_size)\n",
    "        data_loder = creat_data(n_nodes,data_size,batch_size)\n",
    "        valid_loder = creat_data(n_nodes, valid_size, batch_size)\n",
    "        print('DATA CREATED/Problem size:', n_nodes)\n",
    "        trainppo = TrainPPO(n_nodes*2,False,lr,3,hidden_node_dim,1,hidden_edge_dim, epoch,batch_size,conv_laysers,entropy_value,eps_clip,timestep,ppo_epoch)\n",
    "        trainppo.run_train(data_loder,batch_size,valid_loder)\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('vrptest': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08937daeaa3b9cb2c169d4b5e7011e9e651c8b2cb2f9ea96272765da9235f9f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
